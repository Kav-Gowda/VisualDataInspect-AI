# -*- coding: utf-8 -*-
"""VisualDataInspect-AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UO8oAciRYMWO86QsOa691mhbKbNEcOTS
"""

# --- Import Required Libraries ---

# Image processing
import mahotas as mh

# Visualization
import seaborn as sns
from matplotlib import pyplot as plt

# Data handling
import numpy as np
import pandas as pd
import os, glob

# Scikit-learn pipeline & preprocessing
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Classifiers
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import ConfusionMatrixDisplay

# Visualization style
sns.set(style='whitegrid')
plt.rcParams['figure.figsize'] = (10, 6)

print("✅ Libraries imported successfully.")

# --- Data loading utility (no external links; local files or Colab upload) ---
import os
import numpy as np
import pandas as pd
import mahotas as mh
from matplotlib import pyplot as plt

def get_data(folder, file_stem, preview_n=50):
    """
    Reads <folder>/<file_stem>.csv with columns: [filename, class]
    Loads images from <folder>/<file_stem>/<filename>
    Shows up to `preview_n` images (5 per row).
    Returns: np.array of [image, class]
    """
    csv_path = os.path.join(folder, f"{file_stem}.csv")
    img_dir  = os.path.join(folder, file_stem)

    if not os.path.exists(csv_path) or not os.path.isdir(img_dir):
        try:
            # Colab-friendly upload prompt
            from google.colab import files
            missing = []
            if not os.path.exists(csv_path): missing.append(f"{file_stem}.csv")
            if not os.path.isdir(img_dir):  missing.append(f"{file_stem}/ (directory of images)")
            print("Missing:", ", ".join(missing))
            print("Please upload/select the required files/folder.")
            files.upload()  # user will upload CSV; image folder should be uploaded via left file pane (if large)
        except Exception:
            print("Place the CSV and image folder in the working directory:")
            print(f"  - {csv_path}")
            print(f"  - {img_dir}")
            raise

    ds = pd.read_csv(csv_path)
    data = []

    # Preview grid
    plt.rcParams["axes.grid"] = False
    cols = 5
    rows = int(np.ceil(min(preview_n, len(ds)) / cols))
    fig = plt.figure(figsize=(cols * 3.5, rows * 3.5))
    shown = 0

    for idx, (im_name, cls) in enumerate(ds.values):
        img_path = os.path.join(img_dir, im_name)
        image = mh.imread(img_path)
        data.append([image, cls])

        if shown < preview_n:
            ax = plt.subplot(rows, cols, shown + 1)
            ax.imshow(image)
            ax.axis("off")
            ax.set_title(str(cls))
            shown += 1

    plt.tight_layout()
    plt.show()
    return np.array(data, dtype=object)

# --- Load dataset + quick class distribution ---
# folder: parent directory that contains the CSV and the images subfolder
# file_stem: CSV file stem and images subfolder name should match
folder   = "hiroshima-lemon"   # change if different
file_stem = "train_images"     # e.g., train_images.csv and folder train_images/

train = get_data(folder, file_stem, preview_n=50)

# Class distribution (all images)
import seaborn as sns
import pandas as pd

c = pd.DataFrame(train[:, 1], columns=['Class'])
sns.set_style('darkgrid')
ax = sns.countplot(x="Class", data=c)
ax.set_title("Class distribution (all training images)")
plt.show()

# Optional: map numeric classes to names for readability
class_names = {0: 'excellent', 1: 'good', 2: 'processed products', 3: 'disqualified'}

# --- Train/Validation split + per-split class distribution ---
from sklearn.model_selection import train_test_split

im_train, im_val, c_train, c_val = train_test_split(
    train[:, 0], train[:, 1].astype(int), test_size=0.3, shuffle=True, stratify=train[:, 1]
)

print("Train shape:", im_train.shape)  # number of images
print("Val shape:  ", im_val.shape)

# Train dist
ct = pd.DataFrame(c_train, columns=['Train Class'])
ct['Train Class Name'] = ct['Train Class'].map(class_names)
sns.countplot(x="Train Class Name", data=ct,
              order=['excellent', 'good', 'processed products', 'disqualified'])
plt.title("Train split class distribution")
plt.xticks(rotation=15)
plt.show()

# Val dist
cv = pd.DataFrame(c_val, columns=['Val Class'])
cv['Val Class Name'] = cv['Val Class'].map(class_names)
sns.countplot(x="Val Class Name", data=cv,
              order=['excellent', 'good', 'processed products', 'disqualified'])
plt.title("Validation split class distribution")
plt.xticks(rotation=15)
plt.show()

# --- Visualize first 5 images per class from the TRAIN split ---
print("Quality of lemon")
plt.rcParams["axes.grid"] = False

for cls in class_names:
    idxs = np.where(c_train == cls)[0][:5]
    print(class_names[cls])
    fig = plt.figure(figsize=(20, 20))
    for i, im_idx in enumerate(idxs):
        plt.subplot(1, 5, i + 1)
        plt.imshow(im_train[im_idx])
        plt.title(class_names[cls])
        plt.axis("off")
    plt.show()

# --- Image features (Haralick) via mahotas ---
def create_features(im_iterable):
    """
    Convert RGB images -> grayscale -> Haralick features.
    Returns: np.array [n_samples, n_features]
    """
    feats = []
    for image in im_iterable:
        im_grey = mh.colors.rgb2grey(image, dtype=np.uint8)
        feats.append(mh.features.haralick(im_grey).ravel())
    return np.array(feats)

# This may take a few minutes depending on image count
features_train = create_features(im_train)
features_val   = create_features(im_val)
print("features_train:", features_train.shape)
print("features_val:  ", features_val.shape)

# --- Baseline: Logistic Regression (with StandardScaler in a Pipeline) ---
clf = Pipeline([
    ('preproc', StandardScaler()),
    ('classifier', LogisticRegression(max_iter=1000))
])
clf.fit(features_train, c_train)

scores_train = clf.score(features_train, c_train)
scores_val   = clf.score(features_val, c_val)
print('Training DataSet accuracy: {: .1%}'.format(scores_train),
      'Test DataSet accuracy: {: .1%}'.format(scores_val))

ConfusionMatrixDisplay.from_estimator(clf, features_val, c_val)
plt.title("Confusion Matrix — Logistic Regression")
plt.show()

# --- Compare many classical classifiers ---
names = ["Logistic Regression", "Nearest Neighbors", "Linear SVM", "RBF SVM", "Gaussian Process",
         "Decision Tree", "Random Forest", "Neural Net", "AdaBoost",
         "Naive Bayes", "QDA"]

classifiers = [
    LogisticRegression(max_iter=1000),
    KNeighborsClassifier(3),
    SVC(kernel="linear", C=0.025),
    SVC(gamma=2, C=1),
    GaussianProcessClassifier(1.0 * RBF(1.0)),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1, max_iter=1000),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()
]

scores_train = []
scores_val = []

for name, base_clf in zip(names, classifiers):
    print("Fitting:", name)
    pipe = Pipeline([('preproc', StandardScaler()), ('classifier', base_clf)])
    pipe.fit(features_train, c_train)
    scores_train.append(pipe.score(features_train, c_train))
    scores_val.append(pipe.score(features_val, c_val))

# --- Tabular results ---
res = pd.DataFrame(index=names)
res['Test'] = scores_train
res['Validate'] = scores_val
res.index.name = "Classifier accuracy"
pd.options.display.float_format = '{:,.2f}'.format
print(res)

# --- Bar plot comparison ---
x = np.arange(len(names))
width = 0.35

fig, ax = plt.subplots(figsize=(12, 6))
rects1 = ax.bar(x - width/2, scores_train, width, label='Train')
rects2 = ax.bar(x + width/2, scores_val,   width, label='Test')

ax.set_ylabel('Accuracy')
ax.set_title('Accuracy of classifiers')
ax.set_xticks(x)
ax.set_xticklabels(names, rotation=90)
ax.legend()
fig.tight_layout()
plt.show()

# --- Load TEST images (local) ---
def get_test_data(folder, file_stem):
    """
    Reads <folder>/<file_stem>.csv (column 0 = filename) and loads images
    from <folder>/<file_stem>/<filename>.
    """
    csv_path = os.path.join(folder, f"{file_stem}.csv")
    img_dir  = os.path.join(folder, file_stem)
    ds = pd.read_csv(csv_path)
    imgs = []
    for (im_name,) in ds.values:
        img_path = os.path.join(img_dir, im_name)
        imgs.append(mh.imread(img_path))
    return np.array(imgs), ds

d = "hiroshima-lemon"
f = "test_images"
test, test_index_df = get_test_data(d, f)

# Create features for TEST
features_test = create_features(test)
print("test images:", test.shape, "features_test:", features_test.shape)

# --- Classify TEST images using the chosen baseline (Logistic Regression) ---
final_clf = Pipeline([
    ('preproc', StandardScaler()),
    ('classifier', LogisticRegression(max_iter=1000))
]).fit(features_train, c_train)

c_test = final_clf.predict(features_test)
c_test[:10]

# --- Quick visual check: 5 examples per predicted class ---
print("Classification results (examples)")
for cls in class_names:
    idxs = np.where(c_test == cls)[0][:5]
    print(class_names[cls])
    fig = plt.figure(figsize=(20, 20))
    for i, im_idx in enumerate(idxs):
        plt.subplot(1, 5, i + 1)
        plt.imshow(test[im_idx])
        plt.title(class_names[cls])
        plt.axis("off")
    plt.show()

# --- Save results to CSV (filename + predicted class) ---
def save_test_result(folder, file_stem, preds):
    csv_path = os.path.join(folder, f"{file_stem}.csv")
    ds = pd.read_csv(csv_path)
    ds['Class'] = preds
    out_path = 'results.csv'
    ds.to_csv(out_path, index=False)
    print("Saved predictions ->", out_path)

save_test_result(d, f, c_test)